{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a24cb9",
   "metadata": {},
   "source": [
    "The training archive contains 25,000 images of dogs and cats. Train your algorithm on these files and predict the labels for test1.zip (1 = dog, 0 = cat).\n",
    "\n",
    "Data ource: Download data from this source\n",
    "https://www.kaggle.com/competitions/dogs-vs-cats/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b509f15",
   "metadata": {},
   "source": [
    "## 1. Preprocessing (Getting data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da742a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let import some library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f75fc8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sajja\\\\Downloads\\\\dogs-vs-cats'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35bfe58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'dogs-vs-cats',\n",
       " 'DogsVsCats.ipynb',\n",
       " 'img.jpg',\n",
       " 'model1_catsVSdogs_10epoch.h5',\n",
       " 'model_20epochs.h5',\n",
       " 'sampleSubmission.csv',\n",
       " 'test1',\n",
       " 'train']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3038c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let get data set from directory\n",
    "\n",
    "filenames=os.listdir(\"./train\")\n",
    "\n",
    "categories=[]\n",
    "for f_name in filenames:\n",
    "    category=f_name.split('.')[0]   # names are like cat.1 we split with \".\" and select the first part\n",
    "    if category=='dog':\n",
    "        categories.append(1)\n",
    "    else:\n",
    "        categories.append(0)\n",
    "\n",
    "df=pd.DataFrame({\n",
    "    'filename':filenames,\n",
    "    'category':categories\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "127a2570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat.0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat.1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat.10.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat.100.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat.1000.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       filename  category\n",
       "0     cat.0.jpg         0\n",
       "1     cat.1.jpg         0\n",
       "2    cat.10.jpg         0\n",
       "3   cat.100.jpg         0\n",
       "4  cat.1000.jpg         0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab73ef91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12500\n",
       "1    12500\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"].value_counts()  # Data is balanced equal label!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b84ea47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AxesSubplot:xlabel='category', ylabel='count'>,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=df[\"category\"], palette='cubehelix'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63523ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"] = df[\"category\"].replace({0:'cat',1:'dog'})\n",
    "train_df,validate_df = train_test_split(df,test_size=0.20,random_state=101)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "validate_df = validate_df.reset_index(drop=True)\n",
    "\n",
    "total_train=train_df.shape[0]\n",
    "total_validate=validate_df.shape[0]\n",
    "batch_size=15\n",
    "\n",
    "\n",
    "img_width=128\n",
    "img_height=128\n",
    "img_size=(img_width,img_height)\n",
    "Image_Channels=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9137de77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 validated image filenames belonging to 2 classes.\n",
      "Found 5000 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rotation_range=10,\n",
    "                                rescale=1./255,\n",
    "                                shear_range=0.1,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                width_shift_range=0.1,\n",
    "                                height_shift_range=0.1\n",
    "                                )\n",
    "train_generator = train_datagen.flow_from_dataframe(train_df,\n",
    "                                                 \"./train/\",x_col='filename',\n",
    "                                                    y_col='category',\n",
    "                                                 target_size=img_size,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 batch_size=batch_size)\n",
    "\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "                                                            validate_df, \n",
    "                                                            \"./train/\", \n",
    "                                                            x_col='filename',\n",
    "                                                            y_col='category',\n",
    "                                                            target_size=img_size,\n",
    "                                                            class_mode='categorical',\n",
    "                                                            batch_size=batch_size\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe27257",
   "metadata": {},
   "source": [
    "## 2. Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "561a4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dropout,Flatten,Dense,Activation,BatchNormalization\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3),activation='relu',input_shape=(img_width,img_height,Image_Channels)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256,(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15afccd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 126, 126, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 126, 126, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 63, 63, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 63, 63, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 61, 61, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 61, 61, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 30, 30, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 28, 28, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 12, 12, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               4719104   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,112,514\n",
      "Trainable params: 5,110,530\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77a96c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some early stopping criteria\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "earlystop = EarlyStopping(patience = 10)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc',patience = 2,\n",
    "                                            verbose = 1,factor = 0.5,min_lr = 0.00001)\n",
    "\n",
    "callbacks = [earlystop,learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c51093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.7467 - accuracy: 0.6291WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 620s 463ms/step - loss: 0.7467 - accuracy: 0.6291 - val_loss: 0.6648 - val_accuracy: 0.7011 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.5256 - accuracy: 0.7452WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 665s 499ms/step - loss: 0.5256 - accuracy: 0.7452 - val_loss: 1.2884 - val_accuracy: 0.5145 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.4614 - accuracy: 0.7869WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 549s 412ms/step - loss: 0.4614 - accuracy: 0.7869 - val_loss: 0.3692 - val_accuracy: 0.8378 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.4129 - accuracy: 0.8147WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 509s 382ms/step - loss: 0.4129 - accuracy: 0.8147 - val_loss: 0.3809 - val_accuracy: 0.8236 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "1333/1333 [==============================] - ETA: 0s - loss: 0.3842 - accuracy: 0.8307WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "1333/1333 [==============================] - 493s 370ms/step - loss: 0.3842 - accuracy: 0.8307 - val_loss: 0.3974 - val_accuracy: 0.8066 - lr: 0.0010\n",
      "Epoch 6/20\n",
      " 450/1333 [=========>....................] - ETA: 1:06:12 - loss: 0.3540 - accuracy: 0.8507"
     ]
    }
   ],
   "source": [
    "#Let train model for 20 epochs\n",
    "epochs=20   \n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator, \n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=total_validate//batch_size,\n",
    "    steps_per_epoch=total_train//batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31faae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_20epochs.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c3d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history, acc='accuracy', val_acc='val_accuracy'):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    axs[0].plot(range(1,len(model_history.history[acc])+1),model_history.history[acc])\n",
    "    axs[0].plot(range(1,len(model_history.history[val_acc])+1),model_history.history[val_acc])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history[acc])+1),len(model_history.history[acc])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "plot_model_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d507056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model was trained for 20 epochs, for higher accuracy we can increase number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9363b1d",
   "metadata": {},
   "source": [
    "## 3. Testing the model with unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca7b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filenames = os.listdir(\"./test1\")\n",
    "test_df = pd.DataFrame({\n",
    "    'filename': test_filenames\n",
    "})\n",
    "nb_samples = test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc59951",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(test_df,\n",
    "                                                 \"./test1/\",x_col='filename',y_col=None,\n",
    "                                                 target_size=img_size,\n",
    "                                                 class_mode=None,\n",
    "                                                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c2fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict_generator(test_generator, steps=np.ceil(nb_samples/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['category'] = np.argmax(predict, axis=-1) # Selecting the categry which has highest probablity\n",
    "\n",
    "\n",
    "# Let convert prediction to geberator classes\n",
    "label_map = dict((v,k) for k,v in train_generator.class_indices.items())\n",
    "test_df['category'] = test_df['category'].replace(label_map)\n",
    "\n",
    "test_df['category'] = test_df['category'].replace({ 'dog': 1, 'cat': 0 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test = test_df.tail(9)\n",
    "sample_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test = test_df.head(6)\n",
    "sample_test.head()\n",
    "plt.figure(figsize=(12, 24))\n",
    "for index, row in sample_test.iterrows():\n",
    "    filename = row['filename']\n",
    "    category = row['category']\n",
    "    img = load_img(\"./test1/\"+filename, target_size=img_size)\n",
    "    plt.subplot(6, 3, index+1)\n",
    "    plt.imshow(img)\n",
    "    plt.xlabel(filename + '(' + \"{}\".format(category) + ')' )\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c31f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()    # 'img.jpg will be used for prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99546b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "results={\n",
    "    0:'cat',\n",
    "    1:'dog'\n",
    "}\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "im=Image.open(\"img.jpg\")\n",
    "im=im.resize(img_size)\n",
    "im=np.expand_dims(im,axis=0)\n",
    "im=np.array(im)\n",
    "im=im/255\n",
    "pred=np.argmax(model.predict(im), axis=1)\n",
    "print(pred[0],\"-->\", \"This is a:\",results[pred[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e9f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9064856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
